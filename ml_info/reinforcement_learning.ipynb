{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It is an area of machine learning that foucses on how we or something might act in the given environment in order to maximize the given reward.\n",
    "\n",
    "- Study the behaviour of objects in an environment and learn to optimize that\n",
    "\n",
    "- Commonly used in game playing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Decision Processes (MDPs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It gives us a way to formalize sequential decision making"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does MDPs work?\n",
    "\n",
    "- It contains a decision maker called an Agent that interacts with the environment that it is placed in\n",
    "\n",
    "- At each time step the agent will get some representation of the environment state.\n",
    "\n",
    "- Given this representation the agent will then select and action to take\n",
    "\n",
    "- The environment is then transitioned into a new state and the agent is given a reward as a consequence its previous action.\n",
    "\n",
    "- These interactions occur sequentially over time\n",
    "\n",
    "**The Agent's goal is to maximize the cumulative rewards**\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "So, the components of an MDP include:\n",
    "1. Environment  \n",
    "2. Agent    \n",
    "3. States - All the possible States of the Environment    \n",
    "4. Actions - All the actions that the Agent can take on the evironment     \n",
    "5. Rewards - All the rewards that the agent can receive on taking those actions\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected Return\n",
    "\n",
    "The sum of all the future rewards that the agent will get for taking the actions to change the state of the environment.\n",
    "\n",
    "Mathematically:\n",
    "\n",
    "G<sub>t</sub> = R<sub>t+1</sub> + R<sub>t+2</sub> + R<sub>t+3</sub> + ... + R<sub>T</sub>\n",
    "\n",
    "where T is the final time step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Episodic Tasks\n",
    "\n",
    "The tasks where agent-environment interaction naturally breaks up into subsequences, called episodes, are called episodic tasks.\n",
    "\n",
    "&emsp;For example, think about playing a game of pong: <br>\n",
    "- Each new round of the game can be thought of as an episode, and the final time step of an \n",
    "episode occurs when a player scores a point. <br>\n",
    "- Each episode ends in a terminal state at time *T*, which is followed by resetting the environment to some standard starting state or to a random sample from a distribution of possible starting states. <br>\n",
    "- The next episode then begins independently from how the previous episode ended."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuing Tasks\n",
    "\n",
    "The tasks where agent-environment interactions don't break up naturally into episodes, but instead continue without limit. These types of tasks are called continuing tasks.\n",
    "\n",
    "One such example is the Snake Game.\n",
    "\n",
    "Once the game starts it only ends when the snake dies. In this case the terminal state time `T = infinity`. Because of this we cannot use the expected return here as it will also be inifinite.\n",
    "\n",
    "**And to solve this issue we need to refine this term to `Discounted Return`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discounted Return\n",
    "\n",
    "To define the discounted return, we first define the discount rate, γ, to be a number between 0 and 1. The discount rate will be the rate at which we discount future rewards and will determine the present value of future rewards.\n",
    "\n",
    "Mathematically:\n",
    "\n",
    "G<sub>t</sub> = R<sub>t+1</sub> + γR<sub>t+2</sub> + γ<sup>2</sup>R<sub>t+3</sub> + ...\n",
    "\n",
    "`This definition of the discounted return makes it to where our agent will care more about the immediate reward over future rewards since future rewards will be more heavily discounted`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policies\n",
    "\n",
    "A function that maps a given state to the probabilities of selecting each possible action from that state. <br>\n",
    "At time t, the probability of taking action a ∈ A(s) in state s ∈ S is π(a|s).\n",
    "\n",
    "## Value Functions\n",
    "\n",
    "Function of states or state-action pairs that estimate how good it is for an agent to perform a given action in a given state.\n",
    "\n",
    "Value functions are defined w.r.t policies.\n",
    "  - **State-value** function\n",
    "    for policy π, denoted by v<sub>π</sub>, tells us how good any given state is for an agent following policy π. In other words, it gives the value of a state under π\n",
    "\n",
    "    v<sub>π</sub> gives the expected return for starting in state s and following π thereafter.\n",
    "    \n",
    "  - **Action-value** function\n",
    "    for policy π, denoted by q<sub>π</sub>, tells us how good it is to perform an action from a given state under policy π\n",
    "\n",
    "    q<sub>π</sub>(s, a) = E[G<sub>t</sub> | S<sub>t</sub> = S, A<sub>t</sub> = a]\n",
    "\n",
    "    The value of action a in state s under policy π is the expected return starting from state s at time t, taking action a, and following policy π thereafter.\n",
    "\n",
    "    This function is also called **Q-function** and the output from the function for any given state-value pair is called **Q-value**. The letter Q represent quality of taking a given action in a given state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The goal of the reinforcement learning algorithm is to find a policy that will yield a lot of rewards for the agent if the agent indeed follows that policy**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal Policy\n",
    "\n",
    "A policy π is considered to be better than or the same as policy π' if the expected return of π is greater than or equal to the expected return of π' for all states. In other words..<br>\n",
    "\n",
    "π >= π' if and only if v<sub>π</sub>(s) >= v<sub>π'</sub>(s) for all s ∈ S\n",
    "\n",
    "A policy that is better than or at least same as all the other policies is called the `Optimal Policy`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal State-Value\n",
    "\n",
    "The largest expected return achievable by any policy π for each state s.\n",
    "\n",
    "Denoted by v<sub>*</sub>(s) = max v<sub>π</sub>(s)\n",
    "\n",
    "### Optimal Action-Value\n",
    "\n",
    "The largest expected return achievable by any policy π for each possible state-action pair.\n",
    "\n",
    "Denoted by q<sub>*</sub>(s, a) = max q<sub>π</sub>(s, a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bellman Optimality Equation for q<sub>*</sub>\n",
    "\n",
    "One fundamental property of the optimal quality function, q<sub>*</sub> is that it should follow the Bellman inequality.\n",
    "\n",
    "That is,\n",
    "\n",
    "q<sub>*</sub>(s, a) = E[R<sub>t+1</sub> + γ max q<sub>*</sub>(s', a')]\n",
    "\n",
    "It states that for any state-action pair (s, a) at time t, the expected return from starting in state s, selecting an action a and following the optimal policy thereafter is going to be the expected reward we get from taking action a in state s, at time t, which is R<sub>t+1</sub>, plus the maximum expected discounted return that can be achieved from any possible next state-action pair (s', a')."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d946eb87d352ac35db7131733de2e48b5b47ed33604f1e717fd091a1bd312ce0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
