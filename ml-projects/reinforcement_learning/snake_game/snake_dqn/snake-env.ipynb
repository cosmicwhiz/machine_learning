{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "from keras import utils\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flatten\n",
    "from collections import deque\n",
    "from time import time, sleep\n",
    "from keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from enum import Enum\n",
    "\n",
    "utils.disable_interactive_logging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Direction(Enum):\n",
    "    RIGHT = 1\n",
    "    LEFT = 2\n",
    "    UP = 3\n",
    "    DOWN = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SnakeGameEnv:\n",
    "    def __init__(self, grid_size=10):\n",
    "        self.grid_size = grid_size\n",
    "        self.grid = np.zeros((self.grid_size, self.grid_size), dtype=np.uint8)\n",
    "        self.reset()\n",
    "        self.clock_wise = [(0, 1), (1, 0), (0, -1), (-1, 0)]\n",
    "        self.MOVE_PENALTY = -2\n",
    "        self.COLLISION_PENALTY = -200\n",
    "        self.FOOD_REWARD = 100\n",
    "\n",
    "    def generate_food(self):\n",
    "        return random.choice([(x, y) for x in range(self.grid_size) for y in range(self.grid_size) if (x, y) not in self.snake])\n",
    "    \n",
    "    def step(self, action):\n",
    "        # 1-Right Turn   2-Left Turn\n",
    "        idx = self.clock_wise.index(self.direction)\n",
    "        \n",
    "        if action == 1:\n",
    "            next_idx = (idx + 1) % 4\n",
    "            self.direction = self.clock_wise[next_idx]\n",
    "        elif action == 2:\n",
    "            next_idx = (idx - 1) % 4\n",
    "            self.direction = self.clock_wise[next_idx]\n",
    "\n",
    "        new_head = (self.snake[0][0]+self.direction[0], self.snake[0][1]+self.direction[1])\n",
    "\n",
    "        # Check for the collisions\n",
    "        if (\n",
    "            new_head in self.snake\n",
    "            or new_head[0] < 0\n",
    "            or new_head[0] >= self.grid_size\n",
    "            or new_head[1] < 0\n",
    "            or new_head[1] >= self.grid_size\n",
    "        ):\n",
    "            self.done = True\n",
    "            return self.get_image(), self.COLLISION_PENALTY, self.done\n",
    "        \n",
    "        # Move the snake\n",
    "        self.snake.insert(0, new_head)\n",
    "\n",
    "        # Check if the snake ate the food\n",
    "        if new_head == self.food:\n",
    "            self.food = self.generate_food()\n",
    "            reward = self.FOOD_REWARD\n",
    "            self.score += 1\n",
    "        else:\n",
    "            self.snake.pop()\n",
    "            reward = self.MOVE_PENALTY\n",
    "\n",
    "        return self.get_image(), reward, self.done\n",
    "    \n",
    "    def reset(self):\n",
    "        self.snake = [(self.grid_size // 2, self.grid_size // 2)]\n",
    "        self.direction = (1, 0)\n",
    "        self.food = self.generate_food()\n",
    "        self.done = False\n",
    "        self.score = 0\n",
    "        return self.get_image()\n",
    "    \n",
    "    def render(self):\n",
    "        img = self.get_image()\n",
    "        # Resize the RGB image to the desired dimensions\n",
    "        img_resized = cv2.resize(img, (300, 300), interpolation=cv2.INTER_NEAREST)\n",
    "        cv2.imshow(\"image\", np.array(img_resized))\n",
    "        # if cv2.waitKey(50) & 0xFF == ord('q'):\n",
    "        #     return\n",
    "    \n",
    "    def get_image(self):\n",
    "        # Define color mapping\n",
    "        colors = {\n",
    "            1: (255, 0, 0),   # Snake (Green)\n",
    "            2: (0, 255, 0)    # Food (Red)\n",
    "        }\n",
    "\n",
    "        # Create an RGB image\n",
    "        rgb_frame = np.zeros((self.grid_size, self.grid_size, 3), dtype=np.uint8)\n",
    "\n",
    "        for x, y in self.snake:     # Fill the Snake cells\n",
    "            rgb_frame[x, y, :] = colors[1]\n",
    "        rgb_frame[self.food[0], self.food[1], :] = colors[2]    # Fill food cell\n",
    "\n",
    "        img = Image.fromarray(rgb_frame, 'RGB')\n",
    "        return np.array(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=100000)\n",
    "        self.memory_len = 0\n",
    "        self.min_replay_size = 1000\n",
    "        self.replay_frequency = 5\n",
    "        self.batch_size = 16\n",
    "        self.gamma = 0.95\n",
    "        self.epsilon = 1\n",
    "        self.epsilon_decay = 0.999\n",
    "        self.epsilon_min = 0.01\n",
    "        self.model = self.build_model()\n",
    "        self.target_model = self.build_model()\n",
    "        self.target_model_update_frequency = 100    # After every 100 episodes\n",
    "        self.update_target_model()\n",
    "    \n",
    "    def build_model(self):\n",
    "        model = Sequential([\n",
    "            Conv2D(64, (3, 3), input_shape=self.state_size),\n",
    "            MaxPooling2D((2, 2)),\n",
    "            Conv2D(64, (3, 3), activation='relu'),\n",
    "            MaxPooling2D((2, 2)),\n",
    "            Flatten(),\n",
    "            Dense(64, activation='relu'),\n",
    "            Dense(self.action_size, activation='linear')\n",
    "        ]) \n",
    "        model.compile(loss='mse', optimizer=Adam(learning_rate=0.001))\n",
    "        return model\n",
    "    \n",
    "    def update_target_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "    \n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return np.random.randint(0, self.action_size)\n",
    "        return np.argmax(self.model.predict(state)[0])\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        self.memory_len += 1\n",
    "    \n",
    "    def replay(self):\n",
    "        if self.memory_len < self.min_replay_size:\n",
    "            return\n",
    "        \n",
    "        minibatch = random.sample(self.memory, self.batch_size)\n",
    "        \n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target += self.gamma * np.amax(self.target_model.predict(next_state)[0])\n",
    "            \n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SnakeGameEnv(grid_size=10)\n",
    "agent = DQNAgent(state_size=(env.grid_size, env.grid_size, 3), action_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(num_episodes=10000):\n",
    "    rewards_collected = []\n",
    "    for episode in range(1, num_episodes+1):\n",
    "        state = env.reset()\n",
    "        state = state.reshape(-1, *state.shape)/255\n",
    "        episode_reward = 0\n",
    "        step_count = 0\n",
    "        print(f\"Episode {episode}\", end=\" \")\n",
    "\n",
    "        while not env.done:\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            next_state = next_state.reshape(-1, *next_state.shape)/255\n",
    "\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "                \n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            step_count += 1\n",
    "        \n",
    "        rewards_collected.append(episode_reward)\n",
    "\n",
    "        agent.replay()\n",
    "        \n",
    "        print(f\"Reward: {episode_reward} Score: {env.score}\")\n",
    "\n",
    "        # Update target model after every (100) episodes and show the rewards collected\n",
    "        if episode % agent.target_model_update_frequency == 0:\n",
    "            # Show avg reward collected for the last 100 episodes\n",
    "            print(f\"Episode: {episode}  Avg. Reward: {sum(rewards_collected[-100:])*0.01}\")\n",
    "            agent.update_target_model()\n",
    "        \n",
    "        if agent.epsilon > agent.epsilon_min:\n",
    "            agent.epsilon *= agent.epsilon_decay\n",
    "                    \n",
    "        tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(num_episodes=10):\n",
    "    agent.epsilon = 0\n",
    "\n",
    "    for episode in range(1, num_episodes + 1):\n",
    "        state = env.reset()\n",
    "        state = state.reshape(-1, *state.shape)/255\n",
    "        episode_reward = 0\n",
    "\n",
    "        while not env.done:\n",
    "            # Choose actions greedily (exploit) based on the learned Q-values\n",
    "            action = agent.act(state)\n",
    "            print(action)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            next_state = next_state.reshape(-1, *next_state.shape)/255\n",
    "\n",
    "            env.render()\n",
    "            state = next_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent.model = load_model(\"snake_model-1694437083.h5\")\n",
    "# test_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(num_episodes=5000)\n",
    "agent.model.save(f\"snake_model-{time()}.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the game\n",
    "env = SnakeGameEnv(grid_size=10)\n",
    "state = env.reset()\n",
    "action = None\n",
    "while True:\n",
    "    # Implement your DQN agent logic here to take actions and update the game state\n",
    "    # For now, we'll just render the game to visualize it\n",
    "    env.render()\n",
    "    key = cv2.waitKey(100)\n",
    "    # action = random.randint(0, 2)\n",
    "    if key == ord('1'):\n",
    "        action = 1\n",
    "    elif key == ord('2'):\n",
    "        action = 2\n",
    "    else:\n",
    "        action = 0\n",
    "    if key == 27:\n",
    "        break  # ESC to exit\n",
    "    \n",
    "    _, _, done = env.step(action)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional state function from version0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _is_surrounded_by_body(self, direction):\n",
    "    start_row, start_col = self.snake[0][0] + direction[0], self.snake[0][1] + direction[1]\n",
    "    if not 0 <= start_row < self.grid_size or not 0 <= start_col < self.grid_size or self.grid[start_row, start_col] == 1: \n",
    "        return False\n",
    "\n",
    "    moves = [(0, 1), (0, -1), (1, 0), (-1, 0)]\n",
    "    visited = [[False] * self.grid_size for _ in range(self.grid_size)]        \n",
    "    queue = deque([(start_row, start_col)])\n",
    "    \n",
    "    while queue:\n",
    "        row, col = queue.popleft()\n",
    "        visited[row][col] = True\n",
    "        \n",
    "        # If we reach boundary then it is not close loop\n",
    "        if row == 0 or row == self.grid_size - 1 or col == 0 or col == self.grid_size - 1:\n",
    "            return False \n",
    "        \n",
    "        for dr, dc in moves:\n",
    "            r, c = row + dr, col + dc\n",
    "            if 0 <= r < self.grid_size and 0 <= c < self.grid_size and not visited[r][c] and self.grid[r, c] == 0:\n",
    "                queue.append((r, c))\n",
    "    \n",
    "    return True\n",
    "\n",
    "def _open_area(self, direction):\n",
    "    start_row, start_col = self.snake[0][0] + direction[0], self.snake[0][1] + direction[1]\n",
    "    if not 0 <= start_row < self.grid_size or not 0 <= start_col < self.grid_size or self.grid[start_row, start_col] == 1: \n",
    "        return 100\n",
    "    visited = set()\n",
    "    queue = deque([(start_row, start_col)])\n",
    "\n",
    "    area = 0\n",
    "    while queue:\n",
    "        r, c = queue.popleft()\n",
    "\n",
    "        if (r, c) not in visited and 0 <= r < self.grid_size and 0 <= c < self.grid_size and self.grid[r, c] != 1:\n",
    "            visited.add((r, c))\n",
    "            area += 1\n",
    "\n",
    "            # Check adjacent cells\n",
    "            directions = [(0, 1), (0, -1), (1, 0), (-1, 0)]\n",
    "            for dr, dc in directions:\n",
    "                new_r, new_c = r + dr, c + dc\n",
    "                queue.append((new_r, new_c))\n",
    "\n",
    "    return area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cur_dir = self.direction\n",
    "# idx = self.clock_wise.index(cur_dir)\n",
    "# cur_dir_l = self.clock_wise[(idx - 1) % 4]\n",
    "# cur_dir_r = self.clock_wise[(idx + 1) % 4]\n",
    "\n",
    "# area_l = self._find_surrounded_area(cur_dir_l)\n",
    "# area_r = self._find_surrounded_area(cur_dir_r)\n",
    "# if area_l == 100: area_r = 100\n",
    "# elif area_r == 100: area_l = 100\n",
    "\n",
    "# Check if next move head will be surrounded by snake body\n",
    "# self._is_surrounded_by_body(cur_dir),\n",
    "# self._is_surrounded_by_body(cur_dir_l),\n",
    "# self._is_surrounded_by_body(cur_dir_r),\n",
    "\n",
    "# area_l < area_r,\n",
    "# area_r < area_l,"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6ca2bc47f542639c4e55152337bf93c569bfc8366ab825c41e4d3d3446384e64"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
