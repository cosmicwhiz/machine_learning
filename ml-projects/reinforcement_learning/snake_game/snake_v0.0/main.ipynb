{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import random\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras import utils\n",
    "from collections import deque\n",
    "\n",
    "utils.disable_interactive_logging()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Define the Snake Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SnakeEnvironment:\n",
    "    def __init__(self, width, height):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "\n",
    "        # Initialize the game board and snake's position using numpy arrays\n",
    "        self.board = np.zeros((self.height, self.width), dtype=np.int32)\n",
    "        self.snake = np.array([(self.width // 2, self.height // 2)])\n",
    "        self.snakeLength = 1\n",
    "\n",
    "        # Place the snake on the board\n",
    "        self.board[self.snake[0][1], self.snake[0][0]] = 1\n",
    "\n",
    "        # Initialize the food position and place the food on the board\n",
    "        self.food_x, self.food_y = self._generate_food()\n",
    "        self.board[self.food_y, self.food_x] = 2\n",
    "\n",
    "        # Initialize the score\n",
    "        self.score = 0\n",
    "\n",
    "        # Set the number of actions\n",
    "        self.num_actions = 4\n",
    "\n",
    "        self.rendering = False\n",
    "        self.pygame_screen = None\n",
    "\n",
    "    def _generate_food(self):\n",
    "        # Generate food at random position where snake is not present\n",
    "        empty_cells = np.where(self.board == 0)\n",
    "        idx = random.randint(0, empty_cells[0].size-1)\n",
    "        return empty_cells[1][idx], empty_cells[0][idx]\n",
    "    \n",
    "    def reset(self):\n",
    "        # reset the environment to the initial state\n",
    "\n",
    "        # clear the board and reset the snake's position to the center\n",
    "        self.board = np.zeros((self.height, self.width), dtype=np.int32)\n",
    "        self.snake = [(self.width // 2, self.height // 2)]\n",
    "        self.snakeLength = 1\n",
    "\n",
    "        # Place the snake of the board\n",
    "        self.board[self.snake[0][1], self.snake[0][0]] = 1\n",
    "\n",
    "        # Reset the food position\n",
    "        self.food_x, self.food_y = self._generate_food()\n",
    "        self.board[self.food_y, self.food_x] = 2\n",
    "\n",
    "        # Reset the score\n",
    "        self.score = 0\n",
    "        return self.board\n",
    "    \n",
    "    def step(self, action):\n",
    "        # Take the specified action and return the next state, reward, and whether the episode is done\n",
    "\n",
    "        # Example: Let's assume the actions are represented as integers:\n",
    "        # 0 - UP, 1 - DOWN, 2 - LEFT, 3 - RIGHT\n",
    "\n",
    "        # Update the snake's position based on the action\n",
    "        head_x, head_y = self.snake[-1]\n",
    "\n",
    "        if action == 0: # UP\n",
    "            next_head = (head_x, head_y-1)\n",
    "        elif action == 1:  # DOWN\n",
    "            next_head = (head_x, head_y + 1)\n",
    "        elif action == 2:  # LEFT\n",
    "            next_head = (head_x - 1, head_y)\n",
    "        elif action == 3:  # RIGHT\n",
    "            next_head = (head_x + 1, head_y)\n",
    "\n",
    "        # Check for collisions with the walls or the snake's body\n",
    "        if (\n",
    "            next_head[0] < 0\n",
    "            or next_head[0] >= self.width\n",
    "            or next_head[1] < 0\n",
    "            or next_head[1] >= self.height\n",
    "            or next_head in self.snake\n",
    "        ):\n",
    "            # If there's a collision, the episode is done, and the agent receives a negative reward\n",
    "            done = True\n",
    "            reward = -10\n",
    "            return self.board, reward, done\n",
    "        \n",
    "        # If the snake eats the food\n",
    "        if next_head[0] == self.food_x and next_head[1] == self.food_y:\n",
    "            # Increase the score, place a new food, and don't remove the last part of the snake\n",
    "            self.score += 1\n",
    "\n",
    "            # Increase the snake length\n",
    "            self.snakeLength += 1\n",
    "\n",
    "            # Reset the previous food coordinate to 0\n",
    "            self.board[self.food_y, self.food_x] = 0\n",
    "\n",
    "            # Place a new food\n",
    "            self.food_x, self.food_y = self._generate_food()\n",
    "            self.board[self.food_y, self.food_x] = 2\n",
    "        else:\n",
    "            # If the snake didn't eat the food, remove the last part of the snake to move\n",
    "            tail_x, tail_y = self.snake[0]\n",
    "            self.board[tail_y, tail_x] = 0\n",
    "            self.snake.pop(0)\n",
    "        \n",
    "        # Move the snake and update the board\n",
    "        self.snake.append(next_head)\n",
    "        self.board[next_head[1], next_head[0]] = 1\n",
    "\n",
    "        # Check if the game is won (snake covers the entire board)\n",
    "        if self.snakeLength == self.width * self.height - 1:  # -1 for the food cell\n",
    "            done = True\n",
    "            reward = 10  # Positive reward for winning the game\n",
    "            return self.board, reward, done\n",
    "\n",
    "        # The episode is not done yet, and the agent receives a small positive reward for moving\n",
    "        done = False\n",
    "        reward = -1\n",
    "        return self.board, reward, done\n",
    "    \n",
    "    def render(self, message=None):\n",
    "        # Set up pygame window\n",
    "        if not self.rendering:\n",
    "            self.rendering = True\n",
    "            pygame.init()\n",
    "            window_size = (800, 600)\n",
    "            self.pygame_screen = pygame.display.set_mode(window_size)\n",
    "            pygame.display.set_caption(\"Snake Game\")\n",
    "\n",
    "        # Calculate the position to center the playground\n",
    "        playground_x = (800 - self.width * 20) // 2\n",
    "        playground_y = (600 - self.height * 20) // 2\n",
    "\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                self.rendering = False\n",
    "                pygame.quit()\n",
    "                sys.exit()\n",
    "\n",
    "        self.pygame_screen.fill((0, 0, 0))  # Clear the screen\n",
    "\n",
    "        # Draw the message box\n",
    "        if message:\n",
    "            font = pygame.font.Font(None, 36)\n",
    "            text = font.render(message, True, (255, 255, 255))\n",
    "            text_rect = text.get_rect(center=(400, 100))\n",
    "            self.pygame_screen.blit(text, text_rect)\n",
    "\n",
    "        # Draw the snake and food\n",
    "        for y in range(self.height):\n",
    "            for x in range(self.width):\n",
    "                cell_value = self.board[y, x]\n",
    "                if cell_value == 1:  # Snake\n",
    "                    pygame.draw.rect(self.pygame_screen, (0, 255, 0), pygame.Rect(playground_x + x * 20, playground_y + y * 20, 20, 20))\n",
    "                elif cell_value == 2:  # Food\n",
    "                    pygame.draw.rect(self.pygame_screen, (255, 0, 0), pygame.Rect(playground_x + x * 20, playground_y +  y * 20, 20, 20))\n",
    "    \n",
    "        # Draw the virtual boundary around the playground\n",
    "        pygame.draw.rect(self.pygame_screen, (255, 255, 255), pygame.Rect(playground_x, playground_y, self.width * 20, self.height * 20), 2)\n",
    "        pygame.display.flip()\n",
    "    \n",
    "    def close(self):\n",
    "        while True:\n",
    "            for event in pygame.event.get():\n",
    "                if event.type == pygame.QUIT:\n",
    "                    pygame.quit()\n",
    "                    sys.exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Define the DQNAgent class with Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=10000)\n",
    "        self.memory_len = 0\n",
    "        self.gamma = 0.95  # Discount factor for future rewards\n",
    "        self.epsilon = 1.0  # Exploration rate, start with full exploration\n",
    "        self.epsilon_min = 0.01  # Minimum exploration rate\n",
    "        # self.epsilon_decay = 0.995  # Exploration rate decay\n",
    "        self.learning_rate = 0.01  # Learning rate for the neural network\n",
    "        self.target_update_frequency = 40\n",
    "        self.weight_update_frequency = 5\n",
    "        self.model = self._build_model()\n",
    "        self.target_model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        # Neural Network with two fully connected layers\n",
    "        model = Sequential()\n",
    "        model.add(Dense(64, input_shape=self.state_size, activation='relu'))\n",
    "        model.add(Dense(64, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=Adam(learning_rate=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def update_target_model(self):\n",
    "        # Update the target network's weights with the main network's weights\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        # Store the experience in the replay memory\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        self.memory_len += 1\n",
    "\n",
    "    def act(self, state):\n",
    "        # Epsilon-greedy policy to choose the action\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        else:\n",
    "            return np.argmax(self.model.predict(state)[0])\n",
    "    \n",
    "    def epsilon_decay(self, episode, max_episodes):\n",
    "        epsilon_t = 1 - 1.5 * episode / max_episodes\n",
    "        return max(epsilon_t, self.epsilon_min)\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        # Experience replay to train the network\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = reward + self.gamma * np.amax(self.target_model.predict(next_state)[0])\n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Define the Deep-Q-Network(DQN) class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DQN(Model):\n",
    "#     # Initialize the DQN\n",
    "#     def __init__(self, num_actions):\n",
    "#         super(DQN, self).__init__()\n",
    "\n",
    "#         # Flatten the input (2D game board) into 1D array\n",
    "#         self.flatten = layers.Flatten()\n",
    "\n",
    "#         # Define the first hidden layer with 128 neurons and ReLU activation\n",
    "#         self.dense1 = layers.Dense(128, activation='relu')\n",
    "\n",
    "#         # Define the second hidden layer with 128 neurons and ReLU activation\n",
    "#         self.dense2 = layers.Dense(128, activation='relu')\n",
    "\n",
    "#         # Define the output layer with as many neurons as there are actions\n",
    "#         # The output represents the Q-values for each action\n",
    "#         self.output_layer = layers.Dense(num_actions)\n",
    "\n",
    "#         self.optimizer = optimizers.Adam()\n",
    "    \n",
    "#     def __call__(self, inputs):\n",
    "#         x = self.flatten(inputs)\n",
    "#         x = self.dense1(x)\n",
    "#         x = self.dense2(x)\n",
    "#         return self.output_layer(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Implementing the Experience Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ExperienceReplayBuffer:\n",
    "#     def __init__(self, buffer_size):\n",
    "#         # Initialize the experience replay buffer with the specified buffer size\n",
    "#         self.buffer_size = buffer_size\n",
    "#         self.buffer = deque(maxlen=buffer_size)\n",
    "\n",
    "#     def add_experience(self, experience):\n",
    "#         # Add a new experience to the buffer\n",
    "#         # The experience should be a tuple (state, action, reward, next_state, done)\n",
    "#         self.buffer.append(experience)\n",
    "\n",
    "#     def sample(self, batch_size):\n",
    "#         # Sample a batch of experiences randomly from the buffer\n",
    "#         # The batch size specifies how many experiences to sample\n",
    "#         batch = random.sample(self.buffer, batch_size)\n",
    "        \n",
    "#         # Unzip the batch to separate states, actions, rewards, next_states, and dones\n",
    "#         states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        \n",
    "#         return states, actions, rewards, next_states, dones\n",
    "\n",
    "#     def __len__(self):\n",
    "#         # Return the current size of the buffer\n",
    "#         return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Training the Agent with Deep Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Food position: (8, 7)\n",
      "----------- Hit the boundary wall ---------\n",
      "Episode: 1, Total Reward: -47\n",
      "Food position: (1, 1)\n",
      "----------- Hit the boundary wall ---------\n",
      "Episode: 2, Total Reward: -42\n",
      "Food position: (4, 8)\n",
      "----------- Hit the boundary wall ---------\n",
      "Episode: 3, Total Reward: -31\n",
      "Food position: (2, 7)\n",
      "----------- Hit the boundary wall ---------\n",
      "Episode: 4, Total Reward: -20\n",
      "Food position: (7, 8)\n",
      "----------- Hit the boundary wall ---------\n",
      "Episode: 5, Total Reward: -32\n",
      "Food position: (5, 1)\n",
      "----------- Hit the boundary wall ---------\n",
      "Episode: 6, Total Reward: -85\n",
      "Food position: (7, 0)\n",
      "----------- Hit the boundary wall ---------\n",
      "Episode: 7, Total Reward: -35\n",
      "Food position: (4, 4)\n",
      "************* Ate the food! **************\n",
      "Episode: 8, Total Reward: -12\n",
      "Food position: (0, 8)\n",
      "----------- Hit the boundary wall ---------\n",
      "Episode: 9, Total Reward: -58\n",
      "Food position: (0, 4)\n",
      "----------- Hit the boundary wall ---------\n",
      "Episode: 10, Total Reward: -33\n",
      "Food position: (1, 0)\n",
      "----------- Hit the boundary wall ---------\n",
      "Episode: 11, Total Reward: -31\n",
      "Food position: (8, 3)\n",
      "----------- Hit the boundary wall ---------\n",
      "Episode: 12, Total Reward: -51\n",
      "Food position: (6, 4)\n",
      "----------- Hit the boundary wall ---------\n",
      "Episode: 13, Total Reward: -36\n",
      "Food position: (5, 8)\n",
      "----------- Hit the boundary wall ---------\n",
      "Episode: 14, Total Reward: -46\n",
      "Food position: (0, 6)\n",
      "----------- Hit the boundary wall ---------\n",
      "Episode: 15, Total Reward: -99\n",
      "Food position: (8, 5)\n",
      "----------- Hit the boundary wall ---------\n",
      "Episode: 16, Total Reward: -140\n",
      "Food position: (9, 3)\n",
      "----------- Hit the boundary wall ---------\n",
      "Episode: 17, Total Reward: -29\n",
      "Food position: (1, 8)\n",
      "----------- Hit the boundary wall ---------\n",
      "Episode: 18, Total Reward: -69\n",
      "Food position: (2, 0)\n",
      "----------- Hit the boundary wall ---------\n",
      "Episode: 19, Total Reward: -40\n",
      "Food position: (2, 1)\n",
      "----------- Hit the boundary wall ---------\n",
      "Episode: 20, Total Reward: -22\n",
      "Food position: (6, 1)\n",
      "----------- Hit the boundary wall ---------\n",
      "Episode: 21, Total Reward: -63\n",
      "Food position: (9, 8)\n",
      "----------- Hit the boundary wall ---------\n",
      "Episode: 22, Total Reward: -42\n",
      "Food position: (1, 2)\n",
      "----------- Hit the boundary wall ---------\n",
      "Episode: 23, Total Reward: -65\n",
      "Food position: (8, 4)\n",
      "************* Ate the food! **************\n",
      "Episode: 24, Total Reward: -38\n",
      "Food position: (3, 7)\n",
      "----------- Hit the boundary wall ---------\n",
      "Episode: 25, Total Reward: -25\n",
      "Food position: (3, 5)\n",
      "----------- Hit the boundary wall ---------\n",
      "Episode: 26, Total Reward: -34\n",
      "Food position: (3, 7)\n",
      "----------- Hit the boundary wall ---------\n",
      "Episode: 27, Total Reward: -85\n",
      "Food position: (1, 9)\n",
      "************* Ate the food! **************\n",
      "Episode: 28, Total Reward: -74\n",
      "Food position: (6, 7)\n",
      "----------- Hit the boundary wall ---------\n",
      "Episode: 29, Total Reward: -40\n",
      "Food position: (9, 3)\n",
      "----------- Hit the boundary wall ---------\n",
      "Episode: 30, Total Reward: -21\n",
      "Food position: (4, 0)\n",
      "----------- Hit the boundary wall ---------\n",
      "Episode: 31, Total Reward: -26\n",
      "Food position: (2, 0)\n",
      "----------- Hit the boundary wall ---------\n",
      "Episode: 32, Total Reward: -50\n",
      "Food position: (2, 5)\n",
      "----------- Hit the boundary wall ---------\n",
      "Episode: 33, Total Reward: -23\n",
      "Food position: (5, 8)\n",
      "----------- Hit the boundary wall ---------\n",
      "Episode: 34, Total Reward: -25\n",
      "Food position: (9, 8)\n",
      "----------- Hit the boundary wall ---------\n",
      "Episode: 35, Total Reward: -71\n",
      "Food position: (6, 2)\n",
      "----------- Hit the boundary wall ---------\n",
      "Episode: 36, Total Reward: -51\n",
      "Food position: (8, 9)\n",
      "----------- Hit the boundary wall ---------\n",
      "Episode: 37, Total Reward: -30\n",
      "Food position: (4, 3)\n",
      "----------- Hit the boundary wall ---------\n",
      "Episode: 38, Total Reward: -36\n",
      "Food position: (3, 2)\n",
      "----------- Hit the boundary wall ---------\n",
      "Episode: 39, Total Reward: -49\n",
      "Food position: (6, 3)\n",
      "----------- Hit the boundary wall ---------\n",
      "Episode: 40, Total Reward: -18\n",
      "Food position: (5, 2)\n",
      "----------- Hit the boundary wall ---------\n",
      "Episode: 41, Total Reward: -33\n",
      "Food position: (3, 6)\n",
      "----------- Hit the boundary wall ---------\n",
      "Episode: 42, Total Reward: -31\n",
      "Food position: (7, 1)\n",
      "----------- Hit the boundary wall ---------\n",
      "Episode: 43, Total Reward: -24\n",
      "Food position: (6, 8)\n",
      "----------- Hit the boundary wall ---------\n",
      "Episode: 44, Total Reward: -81\n",
      "Food position: (5, 3)\n",
      "----------- Hit the boundary wall ---------\n",
      "Episode: 45, Total Reward: -81\n",
      "Food position: (0, 2)\n",
      "----------- Hit the boundary wall ---------\n",
      "Episode: 46, Total Reward: -21\n",
      "Food position: (8, 0)\n",
      "----------- Hit the boundary wall ---------\n",
      "Episode: 47, Total Reward: -30\n",
      "Food position: (9, 8)\n",
      "----------- Hit the boundary wall ---------\n",
      "Episode: 48, Total Reward: -28\n",
      "Food position: (9, 2)\n",
      "----------- Hit the boundary wall ---------\n",
      "Episode: 49, Total Reward: -40\n",
      "Food position: (3, 2)\n",
      "----------- Hit the boundary wall ---------\n",
      "Episode: 50, Total Reward: -24\n",
      "Food position: (0, 6)\n",
      "----------- Hit the boundary wall ---------\n",
      "Episode: 51, Total Reward: -53\n",
      "Food position: (4, 3)\n",
      "----------- Hit the boundary wall ---------\n",
      "Episode: 52, Total Reward: -16\n",
      "Food position: (5, 6)\n",
      "----------- Hit the boundary wall ---------\n",
      "Episode: 53, Total Reward: -26\n",
      "Food position: (2, 8)\n",
      "----------- Hit the boundary wall ---------\n",
      "Episode: 54, Total Reward: -23\n",
      "Food position: (5, 3)\n",
      "----------- Hit the boundary wall ---------\n",
      "Episode: 55, Total Reward: -28\n",
      "Food position: (4, 9)\n",
      "----------- Hit the boundary wall ---------\n",
      "Episode: 56, Total Reward: -54\n",
      "Food position: (4, 9)\n",
      "----------- Hit the boundary wall ---------\n",
      "Episode: 57, Total Reward: -48\n",
      "Food position: (1, 6)\n",
      "----------- Hit the boundary wall ---------\n",
      "Episode: 58, Total Reward: -25\n",
      "Food position: (5, 6)\n",
      "----------- Hit the boundary wall ---------\n",
      "Episode: 59, Total Reward: -92\n",
      "Food position: (8, 5)\n",
      "----------- Hit the boundary wall ---------\n",
      "Episode: 60, Total Reward: -28\n",
      "Food position: (3, 1)\n",
      "----------- Hit the boundary wall ---------\n",
      "Episode: 61, Total Reward: -16\n",
      "Food position: (5, 7)\n",
      "************* Ate the food! **************\n",
      "Episode: 62, Total Reward: -18\n",
      "Food position: (5, 7)\n",
      "----------- Hit the boundary wall ---------\n",
      "Episode: 63, Total Reward: -34\n",
      "Food position: (0, 2)\n",
      "----------- Hit the boundary wall ---------\n",
      "Episode: 64, Total Reward: -23\n",
      "Food position: (5, 0)\n",
      "----------- Hit the boundary wall ---------\n",
      "Episode: 65, Total Reward: -31\n",
      "Food position: (6, 7)\n",
      "----------- Hit the boundary wall ---------\n",
      "Episode: 66, Total Reward: -35\n",
      "Food position: (5, 1)\n",
      "----------- Hit the boundary wall ---------\n",
      "Episode: 67, Total Reward: -27\n",
      "Food position: (5, 4)\n",
      "----------- Hit the boundary wall ---------\n",
      "Episode: 68, Total Reward: -15\n",
      "Food position: (7, 6)\n",
      "----------- Hit the boundary wall ---------\n",
      "Episode: 69, Total Reward: -15\n",
      "Food position: (0, 4)\n",
      "----------- Hit the boundary wall ---------\n",
      "Episode: 70, Total Reward: -16\n",
      "Food position: (0, 1)\n",
      "----------- Hit the boundary wall ---------\n",
      "Episode: 71, Total Reward: -27\n",
      "Food position: (2, 1)\n",
      "----------- Hit the boundary wall ---------\n",
      "Episode: 72, Total Reward: -29\n",
      "Food position: (3, 2)\n",
      "----------- Hit the boundary wall ---------\n",
      "Episode: 73, Total Reward: -48\n",
      "Food position: (1, 9)\n",
      "----------- Hit the boundary wall ---------\n",
      "Episode: 74, Total Reward: -36\n",
      "Food position: (7, 2)\n",
      "----------- Hit the boundary wall ---------\n",
      "Episode: 75, Total Reward: -38\n",
      "Food position: (3, 2)\n",
      "----------- Hit the boundary wall ---------\n",
      "Episode: 76, Total Reward: -16\n",
      "Food position: (7, 7)\n",
      "----------- Hit the boundary wall ---------\n",
      "Episode: 77, Total Reward: -104\n",
      "Food position: (2, 7)\n",
      "----------- Hit the boundary wall ---------\n",
      "Episode: 78, Total Reward: -22\n",
      "Food position: (5, 0)\n",
      "----------- Hit the boundary wall ---------\n",
      "Episode: 79, Total Reward: -40\n",
      "Food position: (9, 9)\n",
      "----------- Hit the boundary wall ---------\n",
      "Episode: 80, Total Reward: -22\n",
      "Food position: (9, 4)\n",
      "----------- Hit the boundary wall ---------\n",
      "Episode: 81, Total Reward: -24\n",
      "Food position: (0, 9)\n",
      "----------- Hit the boundary wall ---------\n",
      "Episode: 82, Total Reward: -39\n",
      "Food position: (1, 9)\n",
      "----------- Hit the boundary wall ---------\n",
      "Episode: 83, Total Reward: -44\n",
      "Food position: (6, 8)\n",
      "----------- Hit the boundary wall ---------\n",
      "Episode: 84, Total Reward: -54\n",
      "Food position: (9, 3)\n",
      "************* Ate the food! **************\n",
      "Episode: 85, Total Reward: -26\n",
      "Food position: (0, 5)\n",
      "----------- Hit the boundary wall ---------\n",
      "Episode: 86, Total Reward: -31\n",
      "Food position: (1, 1)\n",
      "----------- Hit the boundary wall ---------\n",
      "Episode: 87, Total Reward: -30\n",
      "Food position: (0, 0)\n",
      "----------- Hit the boundary wall ---------\n",
      "Episode: 88, Total Reward: -21\n",
      "Food position: (6, 6)\n",
      "----------- Hit the boundary wall ---------\n",
      "Episode: 89, Total Reward: -15\n",
      "Food position: (6, 4)\n",
      "----------- Hit the boundary wall ---------\n",
      "Episode: 90, Total Reward: -24\n",
      "Food position: (0, 8)\n",
      "----------- Hit the boundary wall ---------\n",
      "Episode: 91, Total Reward: -49\n",
      "Food position: (4, 3)\n",
      "----------- Hit the boundary wall ---------\n",
      "Episode: 92, Total Reward: -32\n",
      "Food position: (8, 5)\n",
      "************* Ate the food! **************\n",
      "Episode: 93, Total Reward: -15\n",
      "Food position: (8, 1)\n",
      "----------- Hit the boundary wall ---------\n",
      "Episode: 94, Total Reward: -19\n",
      "Food position: (3, 8)\n",
      "----------- Hit the boundary wall ---------\n",
      "Episode: 95, Total Reward: -68\n",
      "Food position: (7, 6)\n",
      "----------- Hit the boundary wall ---------\n",
      "Episode: 96, Total Reward: -22\n",
      "Food position: (2, 0)\n",
      "----------- Hit the boundary wall ---------\n",
      "Episode: 97, Total Reward: -57\n",
      "Food position: (0, 2)\n",
      "----------- Hit the boundary wall ---------\n",
      "Episode: 98, Total Reward: -18\n",
      "Food position: (0, 8)\n",
      "----------- Hit the boundary wall ---------\n",
      "Episode: 99, Total Reward: -24\n",
      "Food position: (7, 1)\n",
      "----------- Hit the boundary wall ---------\n",
      "Episode: 100, Total Reward: -156\n",
      "Food position: (9, 2)\n",
      "----------- Hit the boundary wall ---------\n",
      "Episode: 101, Total Reward: -64\n",
      "Food position: (8, 7)\n",
      "----------- Hit the boundary wall ---------\n",
      "Episode: 102, Total Reward: -63\n",
      "Food position: (9, 8)\n",
      "----------- Hit the boundary wall ---------\n",
      "Episode: 103, Total Reward: -21\n",
      "Food position: (6, 7)\n",
      "----------- Hit the boundary wall ---------\n",
      "Episode: 104, Total Reward: -101\n",
      "Food position: (2, 5)\n",
      "----------- Hit the boundary wall ---------\n",
      "Episode: 105, Total Reward: -29\n",
      "Food position: (8, 5)\n",
      "----------- Hit the boundary wall ---------\n",
      "Episode: 106, Total Reward: -45\n",
      "Food position: (6, 2)\n",
      "----------- Hit the boundary wall ---------\n",
      "Episode: 107, Total Reward: -28\n",
      "Food position: (9, 5)\n",
      "----------- Hit the boundary wall ---------\n",
      "Episode: 108, Total Reward: -62\n",
      "Food position: (3, 3)\n",
      "----------- Hit the boundary wall ---------\n",
      "Episode: 109, Total Reward: -17\n",
      "Food position: (5, 6)\n",
      "----------- Hit the boundary wall ---------\n",
      "Episode: 110, Total Reward: -14\n",
      "Food position: (6, 2)\n",
      "----------- Hit the boundary wall ---------\n",
      "Episode: 111, Total Reward: -78\n",
      "Food position: (8, 3)\n",
      "----------- Hit the boundary wall ---------\n",
      "Episode: 112, Total Reward: -19\n",
      "Food position: (4, 1)\n",
      "----------- Hit the boundary wall ---------\n",
      "Episode: 113, Total Reward: -24\n",
      "Food position: (1, 7)\n",
      "----------- Hit the boundary wall ---------\n",
      "Episode: 114, Total Reward: -62\n",
      "Food position: (4, 5)\n",
      "----------- Hit the boundary wall ---------\n",
      "Episode: 115, Total Reward: -21\n",
      "Food position: (4, 2)\n",
      "----------- Hit the boundary wall ---------\n",
      "Episode: 116, Total Reward: -41\n",
      "Food position: (9, 9)\n",
      "----------- Hit the boundary wall ---------\n",
      "Episode: 117, Total Reward: -46\n",
      "Food position: (4, 6)\n",
      "----------- Hit the boundary wall ---------\n",
      "Episode: 118, Total Reward: -75\n",
      "Food position: (1, 4)\n",
      "************* Ate the food! **************\n",
      "Episode: 119, Total Reward: -15\n",
      "Food position: (3, 9)\n",
      "----------- Hit the boundary wall ---------\n",
      "Episode: 120, Total Reward: -27\n",
      "Food position: (3, 6)\n",
      "----------- Hit the boundary wall ---------\n",
      "Episode: 121, Total Reward: -48\n",
      "Food position: (3, 0)\n",
      "----------- Hit the boundary wall ---------\n",
      "Episode: 122, Total Reward: -36\n",
      "Food position: (7, 3)\n",
      "----------- Hit the boundary wall ---------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[103], line 40\u001b[0m\n\u001b[0;32m     38\u001b[0m state_size \u001b[39m=\u001b[39m (\u001b[39m10\u001b[39m, \u001b[39m10\u001b[39m)\n\u001b[0;32m     39\u001b[0m agent \u001b[39m=\u001b[39m DQNAgent(state_size, action_size\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m)\n\u001b[1;32m---> 40\u001b[0m train_agent(\u001b[39m2000\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[103], line 22\u001b[0m, in \u001b[0;36mtrain_agent\u001b[1;34m(num_episodes, batch_size)\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m----------- Hit the boundary wall ---------\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     21\u001b[0m \u001b[39mif\u001b[39;00m episode \u001b[39m%\u001b[39m agent\u001b[39m.\u001b[39mweight_update_frequency \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m agent\u001b[39m.\u001b[39mmemory_len \u001b[39m>\u001b[39m batch_size:\n\u001b[1;32m---> 22\u001b[0m     agent\u001b[39m.\u001b[39;49mreplay(batch_size)\n\u001b[0;32m     24\u001b[0m \u001b[39mif\u001b[39;00m episode \u001b[39m%\u001b[39m agent\u001b[39m.\u001b[39mtarget_update_frequency \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m     25\u001b[0m     agent\u001b[39m.\u001b[39mupdate_target_model()\n",
      "Cell \u001b[1;32mIn[100], line 53\u001b[0m, in \u001b[0;36mDQNAgent.replay\u001b[1;34m(self, batch_size)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m done:\n\u001b[0;32m     52\u001b[0m     target \u001b[39m=\u001b[39m reward \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgamma \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39mamax(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_model\u001b[39m.\u001b[39mpredict(next_state)[\u001b[39m0\u001b[39m])\n\u001b[1;32m---> 53\u001b[0m target_f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mpredict(state)\n\u001b[0;32m     54\u001b[0m target_f[\u001b[39m0\u001b[39m][action] \u001b[39m=\u001b[39m target\n\u001b[0;32m     55\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mfit(state, target_f, epochs\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32md:\\Projects\\Machine Learning\\ml-projects\\snake-v0\\venv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32md:\\Projects\\Machine Learning\\ml-projects\\snake-v0\\venv\\lib\\site-packages\\keras\\src\\engine\\training.py:2550\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   2548\u001b[0m callbacks\u001b[39m.\u001b[39mon_predict_begin()\n\u001b[0;32m   2549\u001b[0m batch_outputs \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 2550\u001b[0m \u001b[39mfor\u001b[39;00m _, iterator \u001b[39min\u001b[39;00m data_handler\u001b[39m.\u001b[39menumerate_epochs():  \u001b[39m# Single epoch.\u001b[39;00m\n\u001b[0;32m   2551\u001b[0m     \u001b[39mwith\u001b[39;00m data_handler\u001b[39m.\u001b[39mcatch_stop_iteration():\n\u001b[0;32m   2552\u001b[0m         \u001b[39mfor\u001b[39;00m step \u001b[39min\u001b[39;00m data_handler\u001b[39m.\u001b[39msteps():\n",
      "File \u001b[1;32md:\\Projects\\Machine Learning\\ml-projects\\snake-v0\\venv\\lib\\site-packages\\keras\\src\\engine\\data_adapter.py:1331\u001b[0m, in \u001b[0;36mDataHandler.enumerate_epochs\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1329\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Yields `(epoch, tf.data.Iterator)`.\"\"\"\u001b[39;00m\n\u001b[0;32m   1330\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_truncate_execution_to_epoch():\n\u001b[1;32m-> 1331\u001b[0m     data_iterator \u001b[39m=\u001b[39m \u001b[39miter\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset)\n\u001b[0;32m   1332\u001b[0m     \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_initial_epoch, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_epochs):\n\u001b[0;32m   1333\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_insufficient_data:  \u001b[39m# Set by `catch_stop_iteration`.\u001b[39;00m\n",
      "File \u001b[1;32md:\\Projects\\Machine Learning\\ml-projects\\snake-v0\\venv\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:506\u001b[0m, in \u001b[0;36mDatasetV2.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    504\u001b[0m \u001b[39mif\u001b[39;00m context\u001b[39m.\u001b[39mexecuting_eagerly() \u001b[39mor\u001b[39;00m ops\u001b[39m.\u001b[39minside_function():\n\u001b[0;32m    505\u001b[0m   \u001b[39mwith\u001b[39;00m ops\u001b[39m.\u001b[39mcolocate_with(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variant_tensor):\n\u001b[1;32m--> 506\u001b[0m     \u001b[39mreturn\u001b[39;00m iterator_ops\u001b[39m.\u001b[39;49mOwnedIterator(\u001b[39mself\u001b[39;49m)\n\u001b[0;32m    507\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    508\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m`tf.data.Dataset` only supports Python-style \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    509\u001b[0m                      \u001b[39m\"\u001b[39m\u001b[39miteration in eager mode or within tf.function.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\Projects\\Machine Learning\\ml-projects\\snake-v0\\venv\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:710\u001b[0m, in \u001b[0;36mOwnedIterator.__init__\u001b[1;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[0;32m    706\u001b[0m   \u001b[39mif\u001b[39;00m (components \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m element_spec \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    707\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    708\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWhen `dataset` is provided, `element_spec` and `components` must \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    709\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mnot be specified.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 710\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_iterator(dataset)\n\u001b[0;32m    712\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_next_call_count \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[1;32md:\\Projects\\Machine Learning\\ml-projects\\snake-v0\\venv\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:749\u001b[0m, in \u001b[0;36mOwnedIterator._create_iterator\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    746\u001b[0m   \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(fulltype\u001b[39m.\u001b[39margs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39margs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39margs) \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(\n\u001b[0;32m    747\u001b[0m       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_output_types)\n\u001b[0;32m    748\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterator_resource\u001b[39m.\u001b[39mop\u001b[39m.\u001b[39mexperimental_set_type(fulltype)\n\u001b[1;32m--> 749\u001b[0m gen_dataset_ops\u001b[39m.\u001b[39;49mmake_iterator(ds_variant, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_iterator_resource)\n",
      "File \u001b[1;32md:\\Projects\\Machine Learning\\ml-projects\\snake-v0\\venv\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py:3451\u001b[0m, in \u001b[0;36mmake_iterator\u001b[1;34m(dataset, iterator, name)\u001b[0m\n\u001b[0;32m   3449\u001b[0m \u001b[39mif\u001b[39;00m tld\u001b[39m.\u001b[39mis_eager:\n\u001b[0;32m   3450\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3451\u001b[0m     _result \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_FastPathExecute(\n\u001b[0;32m   3452\u001b[0m       _ctx, \u001b[39m\"\u001b[39;49m\u001b[39mMakeIterator\u001b[39;49m\u001b[39m\"\u001b[39;49m, name, dataset, iterator)\n\u001b[0;32m   3453\u001b[0m     \u001b[39mreturn\u001b[39;00m _result\n\u001b[0;32m   3454\u001b[0m   \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train_agent(num_episodes, batch_size=32):\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        food_pos = env.food_x, env.food_y\n",
    "        print(f\"Food position: {food_pos}\")\n",
    "        while not done:\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "        if tuple(env.snake[-1]) == tuple(food_pos):\n",
    "            print('************* Ate the food! **************')\n",
    "        else:\n",
    "            print(\"----------- Hit the boundary wall ---------\")\n",
    "\n",
    "        if episode % agent.weight_update_frequency == 0 and agent.memory_len > batch_size:\n",
    "            agent.replay(batch_size)\n",
    "    \n",
    "        if episode % agent.target_update_frequency == 0:\n",
    "            agent.update_target_model()\n",
    "\n",
    "        if agent.epsilon > agent.epsilon_min:\n",
    "            agent.epsilon = agent.epsilon_decay(episode, num_episodes)\n",
    "\n",
    "\n",
    "        # Print the episode number and total reward for monitoring progress\n",
    "        print(f\"Episode: {episode + 1}, Total Reward: {total_reward}\")\n",
    "\n",
    "    print(\"Training completed.\")\n",
    "\n",
    "\n",
    "env = SnakeEnvironment(10, 10)\n",
    "state_size = (10, 10)\n",
    "agent = DQNAgent(state_size, action_size=4)\n",
    "train_agent(2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Evaluate the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent(num_episodes=10):\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        food_pos = env.food_x, env.food_y\n",
    "        print(f\"Food position: {food_pos}\")\n",
    "        message = None\n",
    "        while not done:\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            message = f\"( {env.snake[-1][0]}, {env.snake[-1][1]} )\"\n",
    "            env.render(message)\n",
    "            time.sleep(0.1)\n",
    "\n",
    "        if tuple(env.snake[-1]) == tuple(food_pos):\n",
    "            message = '** Ate the food! **'\n",
    "        else:\n",
    "            message = \"-- Hit the boundary wall --\"\n",
    "\n",
    "        env.render(message)\n",
    "        print(f\"Evaluation Episode: {episode + 1}, Reward: {total_reward}\")\n",
    "        time.sleep(0.8)\n",
    "    \n",
    "    print(\"Finished testing!\")\n",
    "\n",
    "env = SnakeEnvironment(10, 10)\n",
    "evaluate_agent()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "# if __name__ == \"__main__\":\n",
    "#     env = SnakeEnvironment(width=10, height=10)\n",
    "#     num_actions = 4  # UP, DOWN, LEFT, RIGHT\n",
    "#     dqn = DQN(num_actions)\n",
    "#     replay_buffer = ExperienceReplayBuffer(buffer_size=10000)\n",
    "\n",
    "#     train_agent(env, dqn, replay_buffer)\n",
    "    # evaluate_agent(env, dqn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e447c3c037d768f2d97e7514592ce3c261b432983680ab7c2efad868b17366de"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
